{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-70fcc3f61184>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SOM(object):\n",
    "    #To check if the SOM has been trained\n",
    "    _trained = False\n",
    "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
    "\n",
    "        #Assign required variables first\n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        if alpha is None:\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = float(alpha)\n",
    "        if sigma is None:\n",
    "            sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            sigma = float(sigma)\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    " \n",
    "        ##INITIALIZE GRAPH\n",
    "        self._graph = tf.Graph()\n",
    " \n",
    "        ##POPULATE GRAPH WITH NECESSARY COMPONENTS\n",
    "        with self._graph.as_default():\n",
    " \n",
    "            ##VARIABLES AND CONSTANT OPS FOR DATA STORAGE\n",
    " \n",
    "            #Randomly initialized weightage vectors for all neurons,\n",
    "            #stored together as a matrix Variable of size [m*n, dim]\n",
    "            self._weightage_vects = tf.Variable(tf.random_normal(\n",
    "                [m*n, dim]))\n",
    " \n",
    "            #Matrix of size [m*n, 2] for SOM grid locations\n",
    "            #of neurons\n",
    "            self._location_vects = tf.constant(np.array(\n",
    "                list(self._neuron_locations(m, n))))\n",
    " \n",
    "            ##PLACEHOLDERS FOR TRAINING INPUTS\n",
    "            #We need to assign them as attributes to self, since they\n",
    "            #will be fed in during training\n",
    " \n",
    "            #The training vector\n",
    "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
    "            #Iteration number\n",
    "            self._iter_input = tf.placeholder(\"float\")\n",
    " \n",
    "            ##CONSTRUCT TRAINING OP PIECE BY PIECE\n",
    "            #Only the final, 'root' training op needs to be assigned as\n",
    "            #an attribute to self, since all the rest will be executed\n",
    "            #automatically during training\n",
    " \n",
    "            #To compute the Best Matching Unit given a vector\n",
    "            #Basically calculates the Euclidean distance between every\n",
    "            #neuron's weightage vector and the input, and returns the\n",
    "            #index of the neuron which gives the least value\n",
    "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.subtract(self._weightage_vects, tf.stack(\n",
    "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
    "                                  0)\n",
    " \n",
    "            #This will extract the location of the BMU based on the BMU's\n",
    "            #index\n",
    "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
    "                                 np.array([[0, 1]]))\n",
    "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
    "                                          tf.constant(np.array([1, 2]))),\n",
    "                                 [2])\n",
    " \n",
    "            #To compute the alpha and sigma values based on iteration\n",
    "            #number\n",
    "            learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input,\n",
    "                                                  self._n_iterations))\n",
    "            _alpha_op = tf.mul(alpha, learning_rate_op)\n",
    "            _sigma_op = tf.mul(sigma, learning_rate_op)\n",
    " \n",
    "            #Construct the op that will generate a vector with learning\n",
    "            #rates for all neurons, based on iteration number and location\n",
    "            #wrt BMU.\n",
    "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(\n",
    "                self._location_vects, tf.stack(\n",
    "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
    "            neighbourhood_func = tf.exp(tf.neg(tf.div(tf.cast(\n",
    "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "            learning_rate_op = tf.mul(_alpha_op, neighbourhood_func)\n",
    " \n",
    "            #Finally, the op that will use learning_rate_op to update\n",
    "            #the weightage vectors of all neurons based on a particular\n",
    "            #input\n",
    "            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\n",
    "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
    "                                               for i in range(m*n)])\n",
    "            weightage_delta = tf.mul(\n",
    "                learning_rate_multiplier,\n",
    "                tf.subtract(tf.stack([self._vect_input for i in range(m*n)]),\n",
    "                       self._weightage_vects))                                         \n",
    "            new_weightages_op = tf.add(self._weightage_vects,\n",
    "                                       weightage_delta)\n",
    "            self._training_op = tf.assign(self._weightage_vects,\n",
    "                                          new_weightages_op)                                       \n",
    " \n",
    "            ##INITIALIZE SESSION\n",
    "            self._sess = tf.Session()\n",
    " \n",
    "            ##INITIALIZE VARIABLES\n",
    "            init_op = tf.initialize_all_variables()\n",
    "            self._sess.run(init_op)\n",
    " \n",
    "    def _neuron_locations(self, m, n):\n",
    "        #Nested iterations over both dimensions\n",
    "        #to generate all 2-D locations in the map\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                yield np.array([i, j])\n",
    " \n",
    "    def train(self, input_vects):\n",
    "        #Training iterations\n",
    "        for iter_no in range(self._n_iterations):\n",
    "            #Train with each vector one by one\n",
    "            for input_vect in input_vects:\n",
    "                self._sess.run(self._training_op,\n",
    "                               feed_dict={self._vect_input: input_vect,\n",
    "                                          self._iter_input: iter_no})\n",
    " \n",
    "        #Store a centroid grid for easy retrieval later on\n",
    "        centroid_grid = [[] for i in range(self._m)]\n",
    "        self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "        self._locations = list(self._sess.run(self._location_vects))\n",
    "        for i, loc in enumerate(self._locations):\n",
    "            centroid_grid[loc[0]].append(self._weightages[i])\n",
    "        self._centroid_grid = centroid_grid\n",
    " \n",
    "        self._trained = True\n",
    " \n",
    "    def get_centroids(self):\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    "        return self._centroid_grid\n",
    " \n",
    "    def map_vects(self, input_vects):\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect-\n",
    "                                                         self._weightages[x]))\n",
    "            to_return.append(self._locations[min_index])\n",
    " \n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'size' of 'Slice' Op has type int32 that does not match type int64 of argument 'begin'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    511\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[1;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[0;32m    775\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(\"Const_1:0\", shape=(2,), dtype=int32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6647e8691ba8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#Train a 20x30 SOM with 400 iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0msom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSOM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-14e8b98be39b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, m, n, dim, n_iterations, alpha, sigma)\u001b[0m\n\u001b[0;32m     83\u001b[0m                                  np.array([[0, 1]]))\n\u001b[0;32m     84\u001b[0m             bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n\u001b[1;32m---> 85\u001b[1;33m                                           tf.constant(np.array([1, 2]))),\n\u001b[0m\u001b[0;32m     86\u001b[0m                                  [2])\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[1;34m(input_, begin, size, name)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m   \"\"\"\n\u001b[1;32m--> 590\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[1;34m(input, begin, size, name)\u001b[0m\n\u001b[0;32m   4632\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4633\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 4634\u001b[1;33m         \"Slice\", input=input, begin=begin, size=size, name=name)\n\u001b[0m\u001b[0;32m   4635\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    544\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[1;32m--> 546\u001b[1;33m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'size' of 'Slice' Op has type int32 that does not match type int64 of argument 'begin'."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    " \n",
    "#Training inputs for RGBcolors\n",
    "colors = np.array(\n",
    "     [[0., 0., 0.],\n",
    "      [0., 0., 1.],\n",
    "      [0., 0., 0.5],\n",
    "      [0.125, 0.529, 1.0],\n",
    "      [0.33, 0.4, 0.67],\n",
    "      [0.6, 0.5, 1.0],\n",
    "      [0., 1., 0.],\n",
    "      [1., 0., 0.],\n",
    "      [0., 1., 1.],\n",
    "      [1., 0., 1.],\n",
    "      [1., 1., 0.],\n",
    "      [1., 1., 1.],\n",
    "      [.33, .33, .33],\n",
    "      [.5, .5, .5],\n",
    "      [.66, .66, .66]])\n",
    "color_names =['black', 'blue', 'darkblue', 'skyblue',\n",
    "     'greyblue', 'lilac', 'green', 'red',\n",
    "     'cyan', 'violet', 'yellow', 'white',\n",
    "     'darkgrey', 'mediumgrey', 'lightgrey']\n",
    " \n",
    "#Train a 20x30 SOM with 400 iterations\n",
    "som = SOM(20, 30, 3, 400)\n",
    "som.train(colors)\n",
    " \n",
    "#Get output grid\n",
    "image_grid = som.get_centroids()\n",
    " \n",
    "#Map colours to their closest neurons\n",
    "mapped = som.map_vects(colors)\n",
    " \n",
    "#Plot\n",
    "plt.imshow(image_grid)\n",
    "plt.title('Color SOM')\n",
    "for i, m in enumerate(mapped):\n",
    "    plt.text(m[1], m[0], color_names[i], ha='center', va='center',\n",
    "             bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
